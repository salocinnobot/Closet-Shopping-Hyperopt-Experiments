# Fashion-MNIST

Implemetation for Non-Kaggle Data pre-processing yet to be implemented

The project assigned by Professor Wagner involved investigating the vast array of subject areas presented in MAS 4115, and implementing a personal notebook with relevant code pertaining to the experimentation of that subject area. The basis for my project is derived from our recent exploration into the rather untiuitive architecture, and implementation of Convolutional Neural Networks. The intersection of the research into the subject area led me into the use of computationally-aiding optimization such as HyperOpt and Hyperas to optimize hyperparameters with respect to minimzing the loss error function specified for the CNN.

	The dataset upon which the research is based comes from the authors of the benchmark for computer vision data experimentation. The fashion-mnist dataset, created in effort to improve the benchmarking standards for machine learning models of classification, features an image set of 70,000 pixelated images from a set of ten labeled fashion articles ranging from trousers to handbags and sandals. The images contain only one channel,  grayscale, which simplifies the use of multi-channeled filters to single-channeled filters. These images provide the basis of research for our CNN architecture and allow us to isolate our to approach a single-channeled dataset of images.

	The approach for our dataset involves many of the classical pre-processing and preparation techniques which involved downloading our data from Kaggle, and splitting the data into training, training validation, and test sets. To better explain the architecture of CNNs, we featured a section on individual convolutions to help clarify the convoluting, and max-pooling operations the CNN is trained to discover. 

	The CNNs for which Iâ€™ve decided to research for the use of Hyperas, involves the original CNN offered by Professor Wagner in keras_CNN_intro. The model is built upon the basis of a Conv2D-MaxPooling-Conv2D-MaxPooling-Conv2D-MaxPooling-Flatten-Dense Neural Network. The model is sequential, with 3 Conv2D layers each proceeded by a MaxPooling layer. The hyperparameters which we decided to optimize for the Conv2D layer include filters of varying size (8, 16, 32), and kernels of varying size ((2,2),(3,3)). The hyperparameters which we decided to optimize for the MaxPooling layer include pool size ((2,2), (3,3), (4,4)). The other hyperparameters we decided to optimize involve the activation function, optimizer, and consequently the learning rate, and batch implementation. 

	The reflection on the use of Hyperas offers an insight into the importance of understanding pre-constructed principles of CNN architecture before applying optimization techniques. By simply determining factors for which to optimize is not a valid strategy to developing a balanced model of interpretability and accuracy. Although the use of hyperparameter optimization is very provocative when developing models, there are far more important considerations about model selection, prior performance, and mathematical intuition that should be decided upon before using optimizations that leverage computation. 

	The results of our model are as follows:
Activation 0: ReLU, Activation 1: ReLU, Activation 2: ReLU, Batch Size: 128, choiceval (optimizer): sgd, filters 0: 16, filters 1: 16, filters 2: 32, kernel size 0: (2,2), kernel size 1: (2,2), kernel size 2: (3,3), lr 0: 0.01, lr 1: 0.01, lr 2: 0.1, pool size 0: (2,2), pool size 1: (3,3), pool size 2: (4,4).

In conclusion, the importance of mathematical intuition in model-making for our research greatly outweighs the use of computational force when deciding on CNN architecture, yet the underlying expectation to create balanced models of accuracy and interpretability calls for a symbiotic relationship between the two. The potential applications of Hyperas to discover mathematical intuitions to model interpretation are a field of research that intuitively leverage the effectiveness of each technique, and would be a suggestion for further research.
